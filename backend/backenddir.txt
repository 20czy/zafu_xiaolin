从数据库中读取pdf转换为document对象
text spllit
连接向量数据库，向量化处理

检索
构建更高效的检索，利用小模型预处理文本进行分块

为用户的query匹配指定的文档

结构化数据类型主要有三种：
1. xml
2. json
3. csv

将检索获得的文档插入到prompt中输入给大模型

搭建持久化存储，将用户对话记录存储到数据库中
搭建模型输出缓存，汇总功能

创建登陆页面，用户 ==》 会话  会话 ==》 招标书，应标书， 聊天记录

添加自动清理空白会话的功能

python manage.py makemigrations [应用名]

python manage.py makemigrations


                # 创建PromptGenerator实例
                prompt_generator = PromptGenerator()
                
                # 准备基础插槽数据
                slot_data = {
                    "topic": "招标审核",
                    "query": message,
                    "language": "中文"
                }
                
                # 生成包含文档搜索结果的提示（仅用于测试）
                prompt = prompt_generator.generate_prompt_with_search(
                    query=message,
                    slot_data=slot_data,
                    top_k=3
                )
                
                # 仅打印生成的prompt用于测试
                logger.info("[测试] 文档检索和Prompt生成结果:")
                logger.info("-"*30)
                logger.info(f"生成的完整prompt:\n{prompt}")
                logger.info("-"*30)
                
                # 使用普通对话模式
                response = llm.invoke(message)
                
                # 保存AI响应消息
                chat_session.messages.create(
                    content=response.content,
                    is_user=False
                )
                
                logger.info(f"LLM响应:\n{response.content}")
                logger.info("="*50)

将前端上传的文件区分为招标书和应标书 ✅
将rag作为一个工具，提供给控制者在需要的时候调用。
肯定不想要每次请求都调用rag，只在需要时调用

检查一下递归分割的代码，适当增加一下chunk size
阅读Agentic Chunking
优化一下loader

connectLLM 添加温度参数
LLMcontroller 修改为connectLLM

view连上 LLMContorller

@classmethod
    async def _generate_final_response(cls, task: Dict[str, Any], api_result: Dict[str, Any]) -> str:
        """
        Generate a user-friendly response from the API results of the last task
        
        Args:
            task (Dict[str, Any]): The task dictionary containing at least 'task' and 'id' keys.
            - 'task': A string describing the user's request.
            - 'id': A unique identifier for the task.
        api_result (Dict[str, Any]): The result from the API call, containing the data to be formatted.
            
        Returns:
            A string containing the final, user-friendly response.
        """
        
        # Use LLM to create a natural language response from the API data
        try:
            prompt = f"""你是浙江农林大学智能校园助手。请根据以下API返回的数据，生成一个友好、专业的回复。

任务: {task['task']}

API返回数据:
{json.dumps(api_result, ensure_ascii=False, indent=2)}

请将API数据转化为自然、连贯的回复，避免技术性语言，使用适合学生理解的表达方式。如有表格数据，可以使用表格格式展示。
"""
            
            llm = ChatOpenAI(
                model="deepseek-chat",
                openai_api_key=os.getenv("DEEPSEEK_API_KEY"),
                openai_api_base='https://api.deepseek.com',
                temperature=0.7
            )
            # Log the start of response generation
            logger.info(f"Generating final response for task {task['id']}")
            
           # Asynchronously invoke the LLM with a system message and user prompt
            response = await llm.ainvoke([
                {"role": "system", "content": "你是浙江农林大学的专业助手。"},
                {"role": "user", "content": prompt}
            ])

            # Log a preview of the generated response (first 100 characters)
            logger.info(f"Final response generated for task {task['id']}: {response.content[:100]}...")

            return response.content
            
        except Exception as e:
            logger.error(f"Error generating final response: {str(e)}", exc_info=True)
            # Fallback to raw data return
            return f"查询结果: {json.dumps(api_result, ensure_ascii=False)}"

聊天记录的可维护性存疑
生成流式输出的可靠性，异步调用

1. 用户登录信息推荐选课
2. 社团适合案例



