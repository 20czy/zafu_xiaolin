import logging
from dotenv import load_dotenv
from typing import List, Dict, Any, Optional
import os, json, requests
from langchain_openai import ChatOpenAI
import asyncio
from .LLMService import LLMService
from .CampusToolHub import CampusToolHub
from .logger_config import setup_logger


# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

logger = setup_logger(__name__)

class TaskPlaner:
    """
    Central planning LLM that decomposes user requests into subtasks
    """

    # system prompt for task planning
    PLANNING_PROMPT = """ä½ æ˜¯æµ™æ±Ÿå†œæ—å¤§å­¦æ™ºèƒ½æ ¡å›­ç³»ç»Ÿçš„ä¸­å¤®è§„åˆ’å™¨ã€‚ä½ çš„ä»»åŠ¡æ˜¯åœ¨æ ¡å›­åœºæ™¯ä¸‹ï¼Œåˆ†æç”¨æˆ·çš„è¯·æ±‚ï¼Œå¹¶å°†å…¶åˆ†è§£ä¸ºå¯å¤„ç†çš„å­ä»»åŠ¡ã€‚
    
åˆ†æç”¨æˆ·è¯·æ±‚ï¼Œå¹¶ä»¥ä¸‹æ ¼å¼è¿”å›ä»»åŠ¡è®¡åˆ’ï¼š

{{
  "tasks": [
    {{
      "id": 1,
      "task": "å…·ä½“ä»»åŠ¡æè¿°",
      "input": "ç»™è¯¥ä»»åŠ¡çš„è¾“å…¥",
      "depends_on": []
    }},
    {{
      "id": 2,
      "task": "å…·ä½“ä»»åŠ¡æè¿°",
      "input": "ç»™è¯¥ä»»åŠ¡çš„è¾“å…¥",
      "depends_on": [1]  // è¿™è¡¨ç¤ºæ­¤ä»»åŠ¡ä¾èµ–äºä»»åŠ¡1çš„ç»“æœ
    }}
  ],
}}

è§„åˆ™ï¼š
1. æ¯ä¸ªä»»åŠ¡åº”å°½å¯èƒ½ç²¾ç¡®
2. å¦‚æœä»»åŠ¡ä¹‹é—´æœ‰ä¾èµ–å…³ç³»ï¼Œè¯·ä½¿ç”¨depends_onå­—æ®µæŒ‡å®š
3. å¤æ‚è¯·æ±‚åº”åˆ†è§£ä¸ºå¤šä¸ªå­ä»»åŠ¡
4. ç®€å•è¯·æ±‚å¯ä»¥æ˜¯å•ä¸ªä»»åŠ¡

ç”¨æˆ·è¯·æ±‚ï¼š"{user_request}"
    """
    @classmethod
    def create_task_plan(cls, user_request: str) -> Dict[str, Any]:
        """
        Create a plan for handling the user's request
        
        Args:
            user_request: The user's message
            
        Returns:
            Task plan dictionary
        """
        logger.info("å¼€å§‹åˆ›å»ºä»»åŠ¡è®¡åˆ’")
        logger.debug(f"ç”¨æˆ·è¯·æ±‚: {user_request}")
        
        try:
            # Create planning prompt
            prompt = cls.PLANNING_PROMPT.format(
                user_request=user_request
            )
            logger.debug("å·²ç”Ÿæˆè§„åˆ’æç¤ºè¯")

            # Use planning LLM to generate task plan
            logger.info("åˆå§‹åŒ– LLM æ¨¡å‹")
            llm = LLMService.get_llm(model_name='deepseek-chat', temperature=0.2)

            logger.info("å‘ LLM å‘é€è¯·æ±‚")
            planning_response = llm.invoke([
                {"role": "system", "content": prompt},
                {"role": "user", "content": user_request}
            ])
            logger.debug("å·²æ”¶åˆ° LLM å“åº”")

            # Extract JSON from response
            response_text = planning_response.content
            json_match = response_text.strip()
            logger.debug("å¼€å§‹è§£æ LLM å“åº”")

            # Parse the plan
            if "```json" in json_match:
                logger.debug("æ£€æµ‹åˆ° JSON ä»£ç å—ï¼Œè¿›è¡Œæå–")
                json_match = json_match.split("```json")[1].split("```")[0]
            
            task_plan = json.loads(json_match)
            logger.info(f"æˆåŠŸç”Ÿæˆä»»åŠ¡è®¡åˆ’ï¼ŒåŒ…å« {len(task_plan.get('tasks', []))} ä¸ªä»»åŠ¡")
            logger.debug(f"ä»»åŠ¡è®¡åˆ’è¯¦æƒ…: {json.dumps(task_plan, ensure_ascii=False, indent=2)}")
            
            return task_plan

        except json.JSONDecodeError as je:
            logger.error(f"JSON è§£æé”™è¯¯: {str(je)}")
            logger.debug(f"å¯¼è‡´é”™è¯¯çš„å“åº”å†…å®¹: {response_text}")
            return cls._get_fallback_plan(user_request)
        except Exception as e:
            logger.error(f"ä»»åŠ¡è§„åˆ’è¿‡ç¨‹å‡ºé”™: {str(e)}", exc_info=True)
            return cls._get_fallback_plan(user_request)

    @classmethod
    def _get_fallback_plan(cls, user_request: str) -> Dict[str, Any]:
        """
        ç”Ÿæˆé™çº§æ–¹æ¡ˆ
        """
        logger.warning("ä½¿ç”¨é™çº§æ–¹æ¡ˆå¤„ç†è¯·æ±‚")
        return {
            "tasks": [
                {
                    "id": 1,
                    "task": "å¤„ç†ç”¨æˆ·è¯·æ±‚",
                    "input": user_request,
                    "depends_on": []
                }
            ],
            "final_output_task_id": 1
        }
        
class ToolSelector:
    """
    Component that selects appropriate API tools for each task
    """
    
    # System prompt for tool selection
    TOOL_SELECTION_PROMPT = """ä½ æ˜¯æµ™æ±Ÿå†œæ—å¤§å­¦æ™ºèƒ½æ ¡å›­ç³»ç»Ÿçš„å·¥å…·é€‰æ‹©å™¨ã€‚ä½ éœ€è¦ä¸ºæ¯ä¸ªä»»åŠ¡é€‰æ‹©æœ€åˆé€‚çš„å·¥å…·ã€‚
    
<å¯ç”¨å·¥å…·åŠå…¶èƒ½åŠ›>
{tool_capabilities}
</å¯ç”¨å·¥å…·åŠå…¶èƒ½åŠ›>

ä»»åŠ¡è®¡åˆ’ï¼š
{task_plan}

è¯·ä¸ºæ¯ä¸ªä»»åŠ¡é€‰æ‹©æœ€åˆé€‚çš„å·¥å…·ï¼Œå¹¶ä»¥ä¸‹æ ¼å¼è¿”å›å·¥å…·é€‰æ‹©æ–¹æ¡ˆï¼š

{{
  "tool_selections": [
    {{
      "task_id": 1,
      "tool": "æœ€é€‚åˆå¤„ç†æ­¤ä»»åŠ¡çš„å·¥å…·åç§°",
      "params": {{
        "param1": "å€¼1",
        "param2": "å€¼2"
      }},
      "reason": "é€‰æ‹©è¯¥å·¥å…·çš„ç®€çŸ­ç†ç”±"
    }},
    {{
      "task_id": 2,
      "tool": "æœ€é€‚åˆå¤„ç†æ­¤ä»»åŠ¡çš„å·¥å…·åç§°",
      "params": {{
        "param1": "å€¼1",
        "param2": "å€¼2"
      }},
      "reason": "é€‰æ‹©è¯¥å·¥å…·çš„ç®€çŸ­ç†ç”±"
    }}
  ]
}}

è§„åˆ™ï¼š
1. ä¸ºæ¯ä¸ªä»»åŠ¡é€‰æ‹©ä¸€ä¸ªæœ€åˆé€‚çš„APIå·¥å…·
2. ç¡®ä¿æä¾›è¯¥å·¥å…·æ‰€éœ€çš„æ‰€æœ‰å¿…è¦å‚æ•°
3. å¯ä»¥æä¾›å¯é€‰å‚æ•°ä»¥æé«˜ç»“æœå‡†ç¡®æ€§
4. å‚æ•°å€¼åº”åŸºäºä»»åŠ¡æè¿°å’Œç”¨æˆ·è¯·æ±‚æå–
5. å¦‚æœå¿…è¦å‚æ•°åœ¨ç”¨æˆ·è¯·æ±‚ä¸­ä¸æ¸…æ¥šï¼Œä½¿ç”¨åˆç†çš„é»˜è®¤å€¼å¹¶åœ¨reasonä¸­è¯´æ˜
6. å¦‚æœä»»åŠ¡éå¸¸ä¸€èˆ¬ï¼Œå¯ä»¥é€‰æ‹©general_assistantå·¥å…·
7. å¦‚æœä»»åŠ¡ä¾èµ–äºå…¶ä»–ä»»åŠ¡çš„ç»“æœï¼Œå¯ä»¥ä½¿ç”¨å ä½ç¬¦æ ¼å¼ï¼š{{TASK_X_RESULT}}ï¼Œå…¶ä¸­Xæ˜¯ä»»åŠ¡IDï¼Œkeyæ˜¯ç»“æœä¸­çš„é”®
    """
    
    @classmethod
    def select_tools_for_tasks(cls, task_plan: Dict[str, Any]) -> Dict[str, Any]:
        """
        Select appropriate tools for each task in the plan
        
        Args:
            task_plan: The task plan from the TaskPlanner
            
        Returns:
            Tool selection dictionary
        """
        logger.info("å¼€å§‹ä¸ºä»»åŠ¡è®¡åˆ’é€‰æ‹©å·¥å…·")
        logger.debug(f"è¾“å…¥çš„ä»»åŠ¡è®¡åˆ’: {json.dumps(task_plan, ensure_ascii=False)}")
        
        try:
            # Get tool capabilities for selection
            logger.debug("è·å–å·¥å…·èƒ½åŠ›ä¿¡æ¯")
            tool_capabilities = CampusToolHub.get_tool_info_for_planner()
            
            # Create selection prompt
            logger.debug("ç”Ÿæˆå·¥å…·é€‰æ‹©æç¤ºè¯")
            prompt = cls.TOOL_SELECTION_PROMPT.format(
                tool_capabilities=tool_capabilities,
                task_plan=json.dumps(task_plan, ensure_ascii=False, indent=2)
            )
            logger.debug(f"æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
            
            # Use selection LLM to select tools
            logger.info("åˆå§‹åŒ–å·¥å…·é€‰æ‹© LLM æ¨¡å‹")
            llm = LLMService.get_llm(model_name='deepseek-chat', temperature=0.1)
            
            logger.info("å‘ LLM å‘é€å·¥å…·é€‰æ‹©è¯·æ±‚")
            selection_response = llm.invoke([
                {"role": "system", "content": prompt}
            ])
            logger.debug("å·²æ”¶åˆ° LLM å“åº”")
            
            # Extract JSON from response
            response_text = selection_response.content
            json_match = response_text.strip()
            logger.debug("å¼€å§‹è§£æå·¥å…·é€‰æ‹©å“åº”")
            
            # Parse the selections
            if "```json" in json_match:
                logger.debug("æ£€æµ‹åˆ° JSON ä»£ç å—ï¼Œè¿›è¡Œæå–")
                json_match = json_match.split("```json")[1].split("```")[0]
            
            tool_selections = json.loads(json_match)
            num_selections = len(tool_selections.get("tool_selections", []))
            logger.info(f"æˆåŠŸç”Ÿæˆå·¥å…·é€‰æ‹©æ–¹æ¡ˆï¼Œå…± {num_selections} ä¸ªå·¥å…·é€‰æ‹©")
            logger.debug(f"å·¥å…·é€‰æ‹©è¯¦æƒ…: {json.dumps(tool_selections, ensure_ascii=False, indent=2)}")
            
            # Validate tool selections
            for selection in tool_selections.get("tool_selections", []):
                tool_name = selection.get("tool", "unknown")
                task_id = selection.get("task_id")
                logger.debug(f"ä»»åŠ¡ {task_id} é€‰æ‹©äº†å·¥å…·: {tool_name}, åŸå› : {selection.get('reason', 'unknown')}")
            
            return tool_selections
            
        except json.JSONDecodeError as je:
            logger.error(f"å·¥å…·é€‰æ‹© JSON è§£æé”™è¯¯: {str(je)}")
            logger.debug(f"å¯¼è‡´è§£æé”™è¯¯çš„å“åº”å†…å®¹: {response_text}")
            return cls._get_default_selections(task_plan)
        except Exception as e:
            logger.error(f"å·¥å…·é€‰æ‹©è¿‡ç¨‹å‡ºé”™: {str(e)}", exc_info=True)
            return cls._get_default_selections(task_plan)

    @classmethod
    def _get_default_selections(cls, task_plan: Dict[str, Any]) -> Dict[str, Any]:
        """
        ç”Ÿæˆé»˜è®¤çš„å·¥å…·é€‰æ‹©æ–¹æ¡ˆ
        """
        logger.warning("ä½¿ç”¨é»˜è®¤å·¥å…·é€‰æ‹©æ–¹æ¡ˆ")
        default_selections = {
            "tool_selections": [
                {
                    "task_id": task["id"],
                    "tool": "general_assistant",
                    "params": {"query_type": "general", "keywords": task["input"]},
                    "reason": "Default selection due to error"
                }
                for task in task_plan.get("tasks", [])
            ]
        }
        logger.debug(f"ç”Ÿæˆçš„é»˜è®¤é€‰æ‹©æ–¹æ¡ˆ: {json.dumps(default_selections, ensure_ascii=False)}")
        return default_selections

class TaskExecutor:
    """
    Executes tasks according to the plan using selected API tools
    """
    
    @classmethod
    def execute_task(cls, task: Dict[str, Any], tool_selection: Dict[str, Any], task_results: Dict[int, Any]) -> Any:
        """
        Execute a single task with the selected tool
        
        Args:
            task: Task definition
            tool_selection: Selected tool and parameters for this task
            task_results: Results of previously executed tasks
            åŒ…å«å‰ç½®ä»»åŠ¡çš„æ‰§è¡Œç»“æœ
            
        Returns:
            Task execution result
        """
        task_id = task.get("id")
        tool = tool_selection.get("tool", "unknown_tool")
        
        logger.info(f"å¼€å§‹æ‰§è¡Œä»»åŠ¡ ID: {task_id}, ä»»åŠ¡æè¿°: {task.get('task')}, ä½¿ç”¨å·¥å…·: {tool}")
        
        try:
            # Get tool and parameters
            params = tool_selection["params"].copy()
            logger.debug(f"ä»»åŠ¡ {task_id} åˆå§‹å‚æ•°: {params}")
            
            # è§£å†³ä»»åŠ¡çš„å‚æ•°ä¾èµ–é—®é¢˜ï¼Œå³å½“å‰æ‰§è¡Œçš„ä»»åŠ¡ä¾èµ–å‰é¢æ‰§è¡Œä»»åŠ¡ä½œä¸ºå‚æ•°
            for param_key, param_value in params.items():
                if isinstance(param_value, str) and "{" in param_value:
                    import re
                    placeholders = re.findall(r"\{TASK_\d+_RESULT(?:\.\w+)*\}", param_value)
                    logger.debug(f"ä»»åŠ¡ {task_id} å‚æ•° {param_key} åŒ…å«å ä½ç¬¦: {placeholders}")
                    for ph in placeholders:
                        resolved = cls.resolve_placeholder(ph, task_results)
                        params[param_key] = param_value.replace(ph, str(resolved))
            logger.debug(f"ä»»åŠ¡ {task_id} æœ€ç»ˆå‚æ•°: {params}")

            # Call the API
            api_result = CampusToolHub.call_api(tool, params)
            return api_result  # Always return raw API result
            
        except Exception as e:
            logger.error(f"ä»»åŠ¡ {task_id} æ‰§è¡Œé”™è¯¯: {str(e)}", exc_info=True)
            return {"error": f"æ‰§è¡Œä»»åŠ¡æ—¶å‡ºé”™: {str(e)}", "task_id": task_id, "tool": tool}
            
    @classmethod
    def resolve_placeholder(cls, placeholder: str, task_results: Dict[int, Any]) -> Any:
        if not placeholder.startswith("{TASK_") or not placeholder.endswith("}"):
            return placeholder
        try:
            parts = placeholder[1:-1].split(".")
            task_id = int(parts[0].split("_")[1])
            key_path = parts[1:]
            if task_id not in task_results or task_results[task_id].get("status") != "success":
                return f"{{TASK_{task_id}_RESULT_NOT_FOUND}}"
            value = task_results[task_id]["api_result"]
            for key in key_path:
                if isinstance(value, dict):
                    value = value.get(key, f"{{KEY_{key}_NOT_FOUND}}")
                else:
                    return f"{{INVALID_KEY_PATH}}"
            return value
        except Exception as e:
            return f"{{PLACEHOLDER_ERROR: {str(e)}}}"
        
    
def LLMcontroller(message: str, session_id: str = None, chat_history=None):
    """
    the central controller of the chatbot
    Process campus queries using tool-based architecture
    èŠå¤©æœºå™¨äººæ§åˆ¶å™¨
    
    Args:
        message: User message
        session_id: User session ID
        chat_history: Previous conversation history
        
    Returns:
        Response from campus assistant
    """
    try:
        # è·å–å¤„ç†ä¿¡æ¯
        process_info = get_process_info(message, session_id, chat_history)
        
        # ç”Ÿæˆå®Œæ•´å“åº”
        full_response = generate_process_response(process_info)
        return full_response
    
    except Exception as e:
        logger.error(f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}", exc_info=True)
        return "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°äº†é”™è¯¯ã€‚è¯·ç¨åå†è¯•ã€‚"
    
def get_process_info(message: str, session_id: str = None, chat_history=None):
    """
    è·å–å¤„ç†ç”¨æˆ·è¯·æ±‚çš„è¿‡ç¨‹ä¿¡æ¯
    
    Args:
        message: User message
        session_id: User session ID
        chat_history: Previous conversation history
        
    Returns:
        åŒ…å«å¤„ç†è¿‡ç¨‹ä¿¡æ¯çš„å­—å…¸
    """
    # 1. Task Planning: Decompose user request into subtasks
    task_plan = TaskPlaner.create_task_plan(message)
    tasks = task_plan.get("tasks", [])

    # 2. Tool Selection: Select appropriate tools for each task
    tool_selections = ToolSelector.select_tools_for_tasks(task_plan) 
    # Create a mapping of task_id to selected tool
    task_to_tool_map = {
        selection["task_id"]: selection
        for selection in tool_selections.get("tool_selections", [])
    }
    logger.info("Create a mapping of task_id to selected tool", task_to_tool_map)

    # 3. Task Execution: Execute each task with selected tool
    task_results = {}
    for task in tasks:
        task_id = task.get("id")
        deps = task.get("depends_on", [])
        deps_met = all(dep_id in task_results and task_results[dep_id].get("status") == "success" for dep_id in deps)
        if not deps_met:
            task_results[task_id] = {"status": "skipped", "reason": "ä¾èµ–ä»»åŠ¡å¤±è´¥"}
            continue
        
        tool_selection = task_to_tool_map.get(task_id, {
            "tool": "general_assistant",
            "params": {"query_type": "general", "keywords": task.get("input", "")}
        })
        logger.info(f"Selected tool for task {task_id}: {tool_selection}")

        result = TaskExecutor.execute_task(task, tool_selection, task_results)
        if "error" in result:
            task_results[task_id] = {"status": "error", "error": result["error"]}
        else:
            task_results[task_id] = {"status": "success", "api_result": result}

    # 4. è¿”å›å¤„ç†è¿‡ç¨‹ä¿¡æ¯
    process_info = {
        "user_input": message,
        "task_planning": task_plan,
        "tool_selection": tool_selections,
        "task_execution": task_results
    }
    logger.info(f"Process description: {process_info}")
    
    return process_info
    
def generate_process_response(process_info: Dict[str, Any]) -> str:
    prompt = f"""ğŸŒŸâœ¨ æµ™æ±Ÿå†œæ—å¤§å­¦æ™ºèƒ½æ ¡å›­åŠ©æ‰‹ã€Œå†œæ—å°æ™ºã€ä¸Šçº¿å•¦ï¼
Hiï½æˆ‘æ˜¯ä½ çš„ä¸“å±æ ¡å›­å°ç®¡å®¶ï¼Œä»Šå¤©è¦å¸®ä½ è§£å†³ä»€ä¹ˆé—®é¢˜å‘€ï¼Ÿ(â‰§âˆ‡â‰¦)ï¾‰

ğŸ“¢ã€ç›´æ¥å›ç­”åŒºã€‘
ï¼ˆè¿™é‡Œä¼šç›´æ¥å‘Šè¯‰ä½ ç­”æ¡ˆå“Ÿï½ï¼‰

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ­ã€å¹•åå°å‰§åœºã€‘
è®©æˆ‘æ‚„æ‚„å‘Šè¯‰ä½ ï¼Œæˆ‘åˆšæ‰æ˜¯æ€ä¹ˆæ€è€ƒçš„å§ï¼(ï½¡â€¢Ì€á´—-)âœ§

1ï¸âƒ£ ç”¨æˆ·æé—®æ—¶é—´ï¼š
   â¤ åŸè¯æ˜¯ï¼š{process_info['user_input']} 
   ï¼ˆå·å·è¯´ï¼šè¿™ä¸ªé—®é¢˜çœ‹èµ·æ¥å‘¢ï¼ï¼‰

2ï¸âƒ£ æˆ‘çš„å°è®¡åˆ’ï¼š
   ğŸ“ è¡ŒåŠ¨æ¸…å•ï¼š
   {json.dumps(process_info['task_planning'], ensure_ascii=False, indent=2)}
   
3ï¸âƒ£ å·¥å…·è£…å¤‡åº“ï¼š
   ğŸ› ï¸ æˆ‘å·å·ç”¨äº†è¿™äº›å°å·¥å…·ï½
   {json.dumps(process_info['tool_selection'], ensure_ascii=False, indent=2)}

4ï¸âƒ£ ä»»åŠ¡æ‰§è¡Œæ—¥è®°ï¼š
   ğŸš€ é—¯å…³è®°å½•ï¼š
   {json.dumps(process_info['task_execution'], ensure_ascii=False, indent=2)}
   â— æˆåŠŸä»»åŠ¡ï¼šæ‰“å‹¾âœ…
   â— è·³è¿‡ä»»åŠ¡ï¼šä¼‘æ¯â¸ï¸
   â— å¤±è´¥ä»»åŠ¡ï¼šå“å‘€ğŸ˜¢ï¼ˆé™„è§£å†³æ–¹æ¡ˆï¼‰

ğŸ’¡ å°è´´å£«ï¼š
å¦‚æœçœ‹åˆ°çº¢è‰²æ„Ÿå¹å·â—ï¼Œè¯´æ˜æˆ‘éœ€è¦äººå·¥è€å¸ˆå¸®å¿™å•¦ï½
éšæ—¶å¯ä»¥æˆ³æˆ‘ã€Œé‡æ–°æ€è€ƒã€æŒ‰é’®å“¦ï¼

ï¼ˆç»“å°¾å½©è›‹ï¼‰ä»Šå¤©é™ªä½ æ¢ç´¢æ ¡å›­çš„æ—…ç¨‹å°±åˆ°è¿™é‡Œå•¦ï½
è¦è®°å¾—ç»™æˆ‘ç‚¹èµâ¤ï¸+æ”¶è—â­ï¸å“Ÿï¼
"""
    llm = LLMService.get_llm(model_name='deepseek-chat', temperature=0.7)
    response = llm.invoke([{"role": "system", "content": prompt}])
    return response.content
